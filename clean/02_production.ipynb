{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "02_production.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrJj6Ux26IO5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYdIafWsOOUV"
      },
      "source": [
        "First we need to install CLIP and then make sure that we have torch 1.7.1 with CUDA support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djgE7IjbV3sv",
        "outputId": "09dbdc60-5a39-4b78-b8c6-a305024242c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rhsna5j3\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-rhsna5j3\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.41.1)\n",
            "Collecting torch~=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 19kB/s \n",
            "\u001b[?25hCollecting torchvision~=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 278kB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-cp37-none-any.whl size=1368722 sha256=9304b21f73d86fc3b9d0c60a9dc3d61548236fc8b4c384c513bf39b7eec174d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f2ajv0i8/wheels/79/51/d7/69f91d37121befe21d9c52332e04f592e17d1cabc7319b3e09\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=b5704c544df9a6c25f1fcc35d3c5121b807e8e1b4bdaa9ccbf8dbc7f4de24d83\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built clip ftfy\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ftfy, torch, torchvision, clip\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed clip-1.0 ftfy-6.0.3 torch-1.7.1 torchvision-0.8.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "  Found existing installation: torchvision 0.8.2\n",
            "    Uninstalling torchvision-0.8.2:\n",
            "      Successfully uninstalled torchvision-0.8.2\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7_Sk-T7DBEm"
      },
      "source": [
        "We can now load the pretrained public CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6FzbzS6W1R5"
      },
      "source": [
        "import clip\n",
        "import torch\n",
        "\n",
        "# Load the open CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJVrkmy6DVj2"
      },
      "source": [
        "## Download the Precomputed Data\n",
        "\n",
        "In this section the precomputed feature vectors for all photos are downloaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18alAEjEOdSC"
      },
      "source": [
        "In order to compare the photos from the Unsplash dataset to a text query, we need to compute the feature vector of each photo using CLIP. This is a time consuming task, so you can use the feature vectors that I precomputed and uploaded to Google Drive (with the permission from Unsplash). If you want to compute the features yourself, see [here](https://github.com/haltakov/natural-language-image-search#on-your-machine).\n",
        "\n",
        "We need to download two files:\n",
        "* `photo_ids.csv` - a list of the photo IDs for all images in the dataset. The photo ID can be used to get the actual photo from Unsplash.\n",
        "* `features.npy` - a matrix containing the precomputed 512 element feature vector for each photo in the dataset.\n",
        "\n",
        "The files are available on [Google Drive](https://drive.google.com/drive/folders/1WQmedVCDIQKA2R33dkS1f980YsJXRZ-q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAb15OJQZRkt",
        "outputId": "2a788869-28f5-4161-af71-e6b0c6775e57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create a folder for the precomputed features\n",
        "!mkdir unsplash-dataset\n",
        "\n",
        "# Download the photo IDs and the feature vectors\n",
        "!gdown --id 1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj -O unsplash-dataset/photo_ids.csv\n",
        "!gdown --id 1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F -O unsplash-dataset/features.npy\n",
        "\n",
        "# Download from alternative source, if the download doesn't work for some reason (for example download quota limit exceeded)\n",
        "if not Path('unsplash-dataset/photo_ids.csv').exists():\n",
        "  !wget https://transfer.army/api/download/9Z976uW4x_Q/MwBsKNX4 -O unsplash-dataset/photo_ids.csv\n",
        "\n",
        "if not Path('unsplash-dataset/features.npy').exists():\n",
        "  !wget https://transfer.army/api/download/3bNN8ysWN4U/T7815nW8 -O unsplash-dataset/features.npy\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Permission denied: https://drive.google.com/uc?id=1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n",
            "Permission denied: https://drive.google.com/uc?id=1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n",
            "--2021-06-13 15:10:51--  https://transfer.army/api/download/9Z976uW4x_Q/MwBsKNX4\n",
            "Resolving transfer.army (transfer.army)... 18.185.236.87\n",
            "Connecting to transfer.army (transfer.army)|18.185.236.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-06-13 15:10:51 ERROR 404: Not Found.\n",
            "\n",
            "--2021-06-13 15:10:52--  https://transfer.army/api/download/3bNN8ysWN4U/T7815nW8\n",
            "Resolving transfer.army (transfer.army)... 18.185.236.87\n",
            "Connecting to transfer.army (transfer.army)|18.185.236.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-06-13 15:10:52 ERROR 404: Not Found.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVjuUh6oEtPt"
      },
      "source": [
        "After the files are downloaded we need to load them using `pandas` and `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQHcmMo1Ztjz",
        "outputId": "241ec0c8-c89c-4003-b492-2da4e3f199d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the photo IDs\n",
        "photo_ids = pd.read_csv(\"unsplash-dataset/photo_ids.csv\")\n",
        "photo_ids = list(photo_ids['photo_id'])\n",
        "\n",
        "# Load the features vectors\n",
        "photo_features = np.load(\"unsplash-dataset/features.npy\")\n",
        "\n",
        "# Convert features to Tensors: Float32 on CPU and Float16 on GPU\n",
        "if device == \"cpu\":\n",
        "  photo_features = torch.from_numpy(photo_features).float().to(device)\n",
        "else:\n",
        "  photo_features = torch.from_numpy(photo_features).to(device)\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Photos loaded: {len(photo_ids)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-648bf5c5c59d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the photo IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mphoto_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unsplash-dataset/photo_ids.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mphoto_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'photo_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujCKerTnFBk4"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Some important functions for processing the data are defined here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYVNtF-JFtfj"
      },
      "source": [
        "The `encode_search_query` function takes a text description and encodes it into a feature vector using the CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0hmOh3qbcxK"
      },
      "source": [
        "def encode_search_query(search_query):\n",
        "  with torch.no_grad():\n",
        "    # Encode and normalize the search query using CLIP\n",
        "    text_encoded = model.encode_text(clip.tokenize(search_query).to(device))\n",
        "    text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # Retrieve the feature vector\n",
        "  return text_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh1yyJtEGCAX"
      },
      "source": [
        "The `find_best_matches` function compares the text feature vector to the feature vectors of all images and finds the best matches. The function returns the IDs of the best matching photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TcmI5KIbe5F"
      },
      "source": [
        "def find_best_matches(text_features, photo_features, photo_ids, results_count=3):\n",
        "  # Compute the similarity between the search query and each photo using the Cosine similarity\n",
        "  similarities = (photo_features @ text_features.T).squeeze(1)\n",
        "\n",
        "  # Sort the photos by their similarity score\n",
        "  best_photo_idx = (-similarities).argsort()\n",
        "\n",
        "  # Return the photo IDs of the best matches\n",
        "  return [photo_ids[i] for i in best_photo_idx[:results_count]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEmt0F4iHbL0"
      },
      "source": [
        "The `display_photo` function displays a photo from Unsplash given its ID and link to the original photo on Unsplash. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC4HD8cBYOon"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "def display_photo(photo_id):\n",
        "  # Get the URL of the photo resized to have a width of 320px\n",
        "  photo_image_url = f\"https://unsplash.com/photos/{photo_id}/download?w=320\"\n",
        "\n",
        "  # Display the photo\n",
        "  display(Image(url=photo_image_url))\n",
        "\n",
        "  # Display the attribution text\n",
        "  display(HTML(f'Photo on <a target=\"_blank\" href=\"https://unsplash.com/photos/{photo_id}\">Unsplash</a> '))\n",
        "  print()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3ojinZ0JYBC"
      },
      "source": [
        "Putting it all together in one function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvUcljF5JcRn"
      },
      "source": [
        "def search_unslash(search_query, photo_features, photo_ids, results_count=3):\n",
        "  # Encode the search query\n",
        "  text_features = encode_search_query(search_query)\n",
        "\n",
        "  # Find the best matches\n",
        "  best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, results_count)\n",
        "\n",
        "  # Display the best photos\n",
        "  for photo_id in best_photo_ids:\n",
        "    display_photo(photo_id)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cR1Vz5I6ZP8",
        "outputId": "9bf7834b-833d-4882-d166-17389de9f657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "search_query = \"Two dogs playing in the snow\"\n",
        "\n",
        "search_unslash(search_query, photo_features, photo_ids, 3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4c14115352d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msearch_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Two dogs playing in the snow\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msearch_unslash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'photo_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn5G3M75zikX"
      },
      "source": [
        "#hide\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOCydQUNzikb"
      },
      "source": [
        "#hide\n",
        "from fastbook import *\n",
        "from fastai.vision.widgets import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHg_FZeFzikc"
      },
      "source": [
        "# From Model to Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBy1jxN9zikc"
      },
      "source": [
        "## The Practice of Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FQ6BXvHzikd"
      },
      "source": [
        "### Starting Your Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqIK3xLXzikf"
      },
      "source": [
        "### The State of Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRVXhjGszikg"
      },
      "source": [
        "#### Computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8x1OWHgzikg"
      },
      "source": [
        "#### Text (natural language processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nCIXWrUzikh"
      },
      "source": [
        "#### Combining text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR7tFAu2zikh"
      },
      "source": [
        "#### Tabular data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuDsu3ezziki"
      },
      "source": [
        "#### Recommendation systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VgUKy6Cziki"
      },
      "source": [
        "#### Other data types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70J2oRmzikj"
      },
      "source": [
        "### The Drivetrain Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e2rMrdUzikj"
      },
      "source": [
        "## Gathering Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx8TgD5gzikj"
      },
      "source": [
        "# clean\n",
        "To download images with Bing Image Search, sign up at [Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/) for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing 'XXX' with your key and executing it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4f7-gBszikk"
      },
      "source": [
        "import os\n",
        "key = os.environ.get('AZURE_SEARCH_KEY', '1568d1b92fbc9f0892a2f3bc5a1ca01a')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px5CW-Yxzikk",
        "outputId": "e0c18ebf-6c5e-4f44-f9d4-2c7f14f45967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "search_images_bing"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function fastbook.search_images_bing>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8kUNhdMzikk",
        "outputId": "81c563a5-2c00-4359-a3be-8b9e7e48f311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "results = search_images_bing(key, 'grizzly bear')\n",
        "ims = results.attrgot('contentUrl')\n",
        "len(ims)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5b9fdb7b287e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_images_bing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grizzly bear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrgot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'contentUrl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastbook/__init__.py\u001b[0m in \u001b[0;36msearch_images_bing\u001b[0;34m(key, term, min_sz, max_images)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0msearch_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://api.bing.microsoft.com/v7.0/images/search\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Ocp-Apim-Subscription-Key\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: PermissionDenied for url: https://api.bing.microsoft.com/v7.0/images/search?q=grizzly+bear&count=150&min_height=128&min_width=128"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvTKhhhzikl"
      },
      "source": [
        "#hide\n",
        "ims = ['http://3.bp.blogspot.com/-S1scRCkI3vY/UHzV2kucsPI/AAAAAAAAA-k/YQ5UzHEm9Ss/s1600/Grizzly%2BBear%2BWildlife.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zguwu04Ezikl"
      },
      "source": [
        "dest = 'images/grizzly.jpg'\n",
        "download_url(ims[0], dest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgQ-Kcllzikm"
      },
      "source": [
        "im = Image.open(dest)\n",
        "im.to_thumb(128,128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4up9M_lzikm"
      },
      "source": [
        "bear_types = 'grizzly','black','teddy'\n",
        "path = Path('bears')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so-Do3M8zikn"
      },
      "source": [
        "if not path.exists():\n",
        "    path.mkdir()\n",
        "    for o in bear_types:\n",
        "        dest = (path/o)\n",
        "        dest.mkdir(exist_ok=True)\n",
        "        results = search_images_bing(key, f'{o} bear')\n",
        "        download_images(dest, urls=results.attrgot('contentUrl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl2AQRUqzikn"
      },
      "source": [
        "fns = get_image_files(path)\n",
        "fns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtNkm9qxzikn"
      },
      "source": [
        "failed = verify_images(fns)\n",
        "failed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSjDKHg7zikn"
      },
      "source": [
        "failed.map(Path.unlink);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qfRedEWziko"
      },
      "source": [
        "### Sidebar: Getting Help in Jupyter Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld_TdCOlziko"
      },
      "source": [
        "### End sidebar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuugOnfzziko"
      },
      "source": [
        "## From Data to DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVeW-5k1ziko"
      },
      "source": [
        "bears = DataBlock(\n",
        "    blocks=(ImageBlock, CategoryBlock), \n",
        "    get_items=get_image_files, \n",
        "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
        "    get_y=parent_label,\n",
        "    item_tfms=Resize(128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S2keu57zikp"
      },
      "source": [
        "dls = bears.dataloaders(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOyJGlz1zikp"
      },
      "source": [
        "dls.valid.show_batch(max_n=4, nrows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVQjfwB4zikp"
      },
      "source": [
        "bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\n",
        "dls = bears.dataloaders(path)\n",
        "dls.valid.show_batch(max_n=4, nrows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgTkhGXhzikq"
      },
      "source": [
        "bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\n",
        "dls = bears.dataloaders(path)\n",
        "dls.valid.show_batch(max_n=4, nrows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iVCZgRkzikr"
      },
      "source": [
        "bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\n",
        "dls = bears.dataloaders(path)\n",
        "dls.train.show_batch(max_n=4, nrows=1, unique=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBI8I9cGzikr"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYRSXqwgziks"
      },
      "source": [
        "bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\n",
        "dls = bears.dataloaders(path)\n",
        "dls.train.show_batch(max_n=8, nrows=2, unique=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toUoFGYxziks"
      },
      "source": [
        "## Training Your Model, and Using It to Clean Your Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPojcEkJziks"
      },
      "source": [
        "bears = bears.new(\n",
        "    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n",
        "    batch_tfms=aug_transforms())\n",
        "dls = bears.dataloaders(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK03yPbmzikt"
      },
      "source": [
        "learn = cnn_learner(dls, resnet18, metrics=error_rate)\n",
        "learn.fine_tune(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0klxj0Qzikt"
      },
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CKKfFZzzikt"
      },
      "source": [
        "interp.plot_top_losses(5, nrows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0mCsndkziku"
      },
      "source": [
        "cleaner = ImageClassifierCleaner(learn)\n",
        "cleaner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z6ejbM-ziku"
      },
      "source": [
        "#hide\n",
        "# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n",
        "# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R2sNmJAziku"
      },
      "source": [
        "## Turning Your Model into an Online Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YXiXBv6zikw"
      },
      "source": [
        "### Using the Model for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9pcxZAjzikw"
      },
      "source": [
        "learn.export()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tefJ6gHMzikx"
      },
      "source": [
        "path = Path()\n",
        "path.ls(file_exts='.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m37E3UN1zikx"
      },
      "source": [
        "learn_inf = load_learner(path/'export.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZhbkoWFziky"
      },
      "source": [
        "learn_inf.predict('images/grizzly.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JLLIFG4ziky"
      },
      "source": [
        "learn_inf.dls.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrmdHE8ziky"
      },
      "source": [
        "### Creating a Notebook App from the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyFN8i9Lzikz"
      },
      "source": [
        "btn_upload = widgets.FileUpload()\n",
        "btn_upload"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAOZloY5zikz"
      },
      "source": [
        "#hide\n",
        "# For the book, we can't actually click an upload button, so we fake it\n",
        "btn_upload = SimpleNamespace(data = ['images/grizzly.jpg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pKVPm2Zzikz"
      },
      "source": [
        "img = PILImage.create(btn_upload.data[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNdiiXhvzikz"
      },
      "source": [
        "out_pl = widgets.Output()\n",
        "out_pl.clear_output()\n",
        "with out_pl: display(img.to_thumb(128,128))\n",
        "out_pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH_sgdM0zik0"
      },
      "source": [
        "pred,pred_idx,probs = learn_inf.predict(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgK4dRXxzik0"
      },
      "source": [
        "lbl_pred = widgets.Label()\n",
        "lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n",
        "lbl_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcy4PHk3zik1"
      },
      "source": [
        "btn_run = widgets.Button(description='Classify')\n",
        "btn_run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFJDokxuzik1"
      },
      "source": [
        "def on_click_classify(change):\n",
        "    img = PILImage.create(btn_upload.data[-1])\n",
        "    out_pl.clear_output()\n",
        "    with out_pl: display(img.to_thumb(128,128))\n",
        "    pred,pred_idx,probs = learn_inf.predict(img)\n",
        "    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n",
        "\n",
        "btn_run.on_click(on_click_classify)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUVx9utPzik2"
      },
      "source": [
        "#hide\n",
        "#Putting back btn_upload to a widget for next cell\n",
        "btn_upload = widgets.FileUpload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmeCpZtPzik2"
      },
      "source": [
        "VBox([widgets.Label('Select your bear!'), \n",
        "      btn_upload, btn_run, out_pl, lbl_pred])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbKQVUf7zik3"
      },
      "source": [
        "### Turning Your Notebook into a Real App"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUY4Bi61zik4"
      },
      "source": [
        "#hide\n",
        "# !pip install voila\n",
        "# !jupyter serverextension enable --sys-prefix voila "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arsyaOdnzik5"
      },
      "source": [
        "### Deploying your app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHuE-Wq6zik6"
      },
      "source": [
        "## How to Avoid Disaster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svvQefkfzik6"
      },
      "source": [
        "### Unforeseen Consequences and Feedback Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0FFi95Bzik6"
      },
      "source": [
        "## Get Writing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fchs5x2nzik7"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHiqpYrhzik7"
      },
      "source": [
        "1. Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n",
        "1. Where do text models currently have a major deficiency?\n",
        "1. What are possible negative societal implications of text generation models?\n",
        "1. In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n",
        "1. What kind of tabular data is deep learning particularly good at?\n",
        "1. What's a key downside of directly using a deep learning model for recommendation systems?\n",
        "1. What are the steps of the Drivetrain Approach?\n",
        "1. How do the steps of the Drivetrain Approach map to a recommendation system?\n",
        "1. Create an image recognition model using data you curate, and deploy it on the web.\n",
        "1. What is `DataLoaders`?\n",
        "1. What four things do we need to tell fastai to create `DataLoaders`?\n",
        "1. What does the `splitter` parameter to `DataBlock` do?\n",
        "1. How do we ensure a random split always gives the same validation set?\n",
        "1. What letters are often used to signify the independent and dependent variables?\n",
        "1. What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\n",
        "1. What is data augmentation? Why is it needed?\n",
        "1. What is the difference between `item_tfms` and `batch_tfms`?\n",
        "1. What is a confusion matrix?\n",
        "1. What does `export` save?\n",
        "1. What is it called when we use a model for getting predictions, instead of training?\n",
        "1. What are IPython widgets?\n",
        "1. When might you want to use CPU for deployment? When might GPU be better?\n",
        "1. What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n",
        "1. What are three examples of problems that could occur when rolling out a bear warning system in practice?\n",
        "1. What is \"out-of-domain data\"?\n",
        "1. What is \"domain shift\"?\n",
        "1. What are the three steps in the deployment process?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acwgUTTvzik8"
      },
      "source": [
        "### Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg9uRuSXzik8"
      },
      "source": [
        "1. Consider how the Drivetrain Approach maps to a project or problem you're interested in.\n",
        "1. When might it be best to avoid certain types of data augmentation?\n",
        "1. For a project you're interested in applying deep learning to, consider the thought experiment \"What would happen if it went really, really well?\"\n",
        "1. Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you're interested in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYyA2Jf-zik9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}